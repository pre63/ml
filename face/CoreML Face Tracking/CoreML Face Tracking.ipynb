{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install**\n",
    "```fish\n",
    "python3 -m venv  .venv\n",
    "source .venv/bin/activate.fish\n",
    "pip install -r face/CoreML\\ Face\\ Tracking/requirements.txt\n",
    "python -m ipykernel install --user --name=.venv\n",
    "```\n",
    "\n",
    "http://localhost:8889/tree?token=57711336f39e4c46af81a847eb15b251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.4.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "Torch version 2.3.0+cu121 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import  transforms\n",
    "from torchvision.datasets import LFWPeople\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import coremltools as ct\n",
    "\n",
    "import pickle\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"4b28b7410bc92bce660b446e56bd56f33dca3e44\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to a common size\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pretrained models\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "lfw_people_train = LFWPeople(root='./data', split='train', transform=transform, download=True)\n",
    "lfw_people_test = LFWPeople(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(lfw_people_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(lfw_people_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon2/dev/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/simon2/dev/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# Anchor generator for the FPN which by default has 5 feature maps\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# Define the model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,  # Background and face\n",
    "                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./models'):\n",
    "    os.mkdir('./models')\n",
    "\n",
    "def saveml(model, filepath):\n",
    "    example = torch.rand(1, 3, 128, 128).to(device)\n",
    "    traced = torch.jit.trace(model, example) \n",
    "    traced.save(filepath)\n",
    "\n",
    "\n",
    "def save(model, filepath):\n",
    "    # Extract the state dictionary from the model\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # Open a file in binary-write mode\n",
    "    with open(filepath, 'wb') as file:\n",
    "        # Use pickle to dump the model state dictionary into the file\n",
    "        pickle.dump(model_state_dict, file)\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    wandb.log({\"test_accuracy\": accuracy, \"test_precision\": precision, \"test_recall\": recall, \"test_f1_score\": f1})\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wandb(config):\n",
    "    wandb.init(project='face', config=config)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    initialize_wandb({\n",
    "        'learning_rate': 0.001,\n",
    "        'epochs': num_epochs,\n",
    "        'batch_size': 64,\n",
    "        'optimizer': 'Adam'\n",
    "    })\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Converting outputs to probabilities and predictions\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(probs, 1)\n",
    "\n",
    "            # Calculate metrics per batch\n",
    "            accuracy = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "            precision = precision_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro', zero_division=0)\n",
    "            recall = recall_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro', zero_division=0)\n",
    "            f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro', zero_division=0)\n",
    "\n",
    "            # Log metrics per batch\n",
    "            wandb.log({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_accuracy\": accuracy,\n",
    "                \"batch_precision\": precision,\n",
    "                \"batch_recall\": recall,\n",
    "                \"batch_f1_score\": f1\n",
    "            })\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} completed, Loss: {loss.item()}')\n",
    "        wandb.log({\"epoch\": epoch + 1, \"epoch_loss\": loss.item()})\n",
    "        saveml(model, './models/model.mlmodel')\n",
    "        save(model, './models/model.pth')\n",
    "\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpre63\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simon2/dev/wandb/run-20240507_175932-ixbmtkf5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pre63/face/runs/ixbmtkf5' target=\"_blank\">faithful-wind-5</a></strong> to <a href='https://wandb.ai/pre63/face' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pre63/face' target=\"_blank\">https://wandb.ai/pre63/face</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pre63/face/runs/ixbmtkf5' target=\"_blank\">https://wandb.ai/pre63/face/runs/ixbmtkf5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon2/dev/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/simon2/dev/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed, Loss: 7.754355430603027\n",
      "Epoch 2/5 completed, Loss: 7.149600505828857\n",
      "Epoch 3/5 completed, Loss: 7.164018630981445\n",
      "Epoch 4/5 completed, Loss: 6.79644250869751\n",
      "Epoch 5/5 completed, Loss: 4.920729637145996\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr=0.001), num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon2/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/simon2/dev/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message with type 'CoreML.Specification.Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/dach/dev/ml/face/CoreML Face Tracking/CoreML Face Tracking.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m download_image(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Example usage\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m bounding_boxes \u001b[39m=\u001b[39m predict_image(\u001b[39m'\u001b[39;49m\u001b[39m./models/model.mlmodel\u001b[39;49m\u001b[39m'\u001b[39;49m, sample_image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBounding Boxes:\u001b[39m\u001b[39m\"\u001b[39m, bounding_boxes)\n",
      "\u001b[1;32m/Users/dach/dev/ml/face/CoreML Face Tracking/CoreML Face Tracking.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the CoreML model, prepare the image, and make a prediction.\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Load the CoreML model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m coremltools\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mMLModel(model_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Load and prepare the image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dach/dev/ml/face/CoreML%20Face%20Tracking/CoreML%20Face%20Tracking.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m image \u001b[39m=\u001b[39m load_image(image_path, (\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m))  \u001b[39m# Resize to the input size expected by the model\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/coremltools/models/model.py:359\u001b[0m, in \u001b[0;36mMLModel.__init__\u001b[0;34m(self, model, is_temp_package, mil_program, skip_model_load, compute_units, weights_dir)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_temp_package \u001b[39m=\u001b[39m is_temp_package\n\u001b[1;32m    358\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_weights_dir \u001b[39m=\u001b[39m _try_get_weights_dir_path(model)\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__proxy__, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spec, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_framework_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_proxy_and_spec(\n\u001b[1;32m    360\u001b[0m         model, compute_units, skip_model_load\u001b[39m=\u001b[39;49mskip_model_load,\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, _proto\u001b[39m.\u001b[39mModel_pb2\u001b[39m.\u001b[39mModel):\n\u001b[1;32m    363\u001b[0m     \u001b[39mif\u001b[39;00m does_model_contain_mlprogram(model):\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/coremltools/models/model.py:405\u001b[0m, in \u001b[0;36mMLModel._get_proxy_and_spec\u001b[0;34m(self, filename, compute_units, skip_model_load)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_proxy_and_spec\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m                         filename: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    401\u001b[0m                         compute_units: _ComputeUnit,\n\u001b[1;32m    402\u001b[0m                         skip_model_load: _Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    404\u001b[0m     filename \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(filename)\n\u001b[0;32m--> 405\u001b[0m     specification \u001b[39m=\u001b[39m _load_spec(filename)\n\u001b[1;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m _MLModelProxy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_model_load:\n\u001b[1;32m    408\u001b[0m \n\u001b[1;32m    409\u001b[0m         \u001b[39m# check if the version is supported\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         engine_version \u001b[39m=\u001b[39m _MLModelProxy\u001b[39m.\u001b[39mmaximum_supported_specification_version()\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/coremltools/models/utils.py:226\u001b[0m, in \u001b[0;36mload_spec\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    224\u001b[0m spec \u001b[39m=\u001b[39m _proto\u001b[39m.\u001b[39mModel_pb2\u001b[39m.\u001b[39mModel()\n\u001b[1;32m    225\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(specfile, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 226\u001b[0m     spec\u001b[39m.\u001b[39;49mParseFromString(f\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m spec\n",
      "\u001b[0;31mDecodeError\u001b[0m: Error parsing message with type 'CoreML.Specification.Model'"
     ]
    }
   ],
   "source": [
    "import coremltools\n",
    "import PIL.Image\n",
    "\n",
    "def load_image(image_path, resize_to):\n",
    "    \"\"\"Load an image from the file system and resize it.\"\"\"\n",
    "    image = PIL.Image.open(image_path)\n",
    "    image = image.resize(resize_to, PIL.Image.ANTIALIAS)\n",
    "    return image\n",
    "\n",
    "def prepare_image(image):\n",
    "    \"\"\"Prepare the image as required by the CoreML model.\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "def predict_image(model_path, image_path):\n",
    "    \"\"\"Load the CoreML model, prepare the image, and make a prediction.\"\"\"\n",
    "    # Load the CoreML model\n",
    "    model = coremltools.models.MLModel(model_path)\n",
    "    \n",
    "    # Load and prepare the image\n",
    "    image = load_image(image_path, (128, 128))  # Resize to the input size expected by the model\n",
    "    image = prepare_image(image)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict({'image': image})\n",
    "    \n",
    "    # Extract bounding boxes from the prediction\n",
    "    if 'coordinates' in prediction:\n",
    "        bounding_boxes = prediction['coordinates']\n",
    "    else:\n",
    "        bounding_boxes = None\n",
    "    \n",
    "    return bounding_boxes\n",
    "\n",
    "# download image fromthe web\n",
    "\n",
    "sample_image_path = \"./data/sample.jpg\"\n",
    "\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    file = open(sample_image_path, \"wb\")\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "url = 'https://cdn.britannica.com/79/173779-050-2FC54270/Andrew-Grove-Robert-Noyce-Gordon-Moore-1978.jpg'\n",
    "download_image(url)\n",
    "\n",
    "# Example usage\n",
    "bounding_boxes = predict_image('./models/model.mlmodel', sample_image_path)\n",
    "print(\"Bounding Boxes:\", bounding_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
